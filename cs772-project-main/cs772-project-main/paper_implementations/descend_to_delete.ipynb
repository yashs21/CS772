{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "https://github.com/ChrisWaites/descent-to-delete/"
      ],
      "metadata": {
        "id": "mRSvf2QZfYd3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZcTSHhB-dJFR",
        "outputId": "96b3de7b-8afd-4928-8701-14a9a1b16c68"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing updates...\n",
            "Accuracy: 0.9850\n",
            "\n",
            "Epsilon: 5, Delta: 1e-06, Sigma: 0.2167\n",
            "Accuracy (published): 0.9450\n"
          ]
        }
      ],
      "source": [
        "import jax.numpy as np\n",
        "from jax import grad, nn, random, jit\n",
        "from jax.example_libraries  .optimizers import l2_norm, clip_grads\n",
        "from jax.numpy import linalg\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "def predict(W, X):\n",
        "  \"\"\"Forward propagation for logistic regression.\"\"\"\n",
        "  return nn.sigmoid(np.dot(X, W))\n",
        "\n",
        "def loss(W, X, y, l2=0.):\n",
        "  \"\"\"Binary cross entropy loss with l2 regularization.\"\"\"\n",
        "  y_hat = predict(W, X)\n",
        "  bce = y * np.log(y_hat) + (1. - y) * np.log(1. - y_hat)\n",
        "  return -np.mean(bce) + l2 * l2_norm(W)\n",
        "\n",
        "def unit_projection(W):\n",
        "  \"\"\"Projects model parameters to have at most l2 norm of 1.\"\"\"\n",
        "  return clip_grads(W, 1)\n",
        "\n",
        "def step(W, X, y, l2=0., proj=unit_projection):\n",
        "  \"\"\"A single step of projected gradient descent.\"\"\"\n",
        "  g = grad(loss)(W, X, y, l2)\n",
        "  W = W - 0.5 * g\n",
        "  W = proj(W)\n",
        "  return W\n",
        "\n",
        "def train(W, X, y, l2=0., iters=1):\n",
        "  \"\"\"Simply executes several model parameter steps.\"\"\"\n",
        "  for i in range(iters):\n",
        "    W = step(W, X, y, l2)\n",
        "  return W\n",
        "\n",
        "def process_update(W, X, y, update, train):\n",
        "  \"\"\"\n",
        "  Updates the dataset according to some update function (e.g. append datum, delete datum) then\n",
        "  finetunes the model on the resulting dataset according to some given training function.\n",
        "  \"\"\"\n",
        "  X, y = update(X, y)\n",
        "  W = train(W, X, y)\n",
        "  return W, X, y\n",
        "\n",
        "def process_updates(W, X, y, updates, train):\n",
        "  \"\"\"Processes a sequence of updates.\"\"\"\n",
        "  for update in updates:\n",
        "    W, X, y = process_update(W, X, y, update, train)\n",
        "  return W, X, y\n",
        "\n",
        "def compute_sigma(num_examples, iterations, lipshitz, strong, epsilon, delta):\n",
        "  \"\"\"Theorem 3.1 https://arxiv.org/pdf/2007.02923.pdf\"\"\"\n",
        "  gamma = (smooth - strong) / (smooth + strong)\n",
        "  numerator = 4 * np.sqrt(2) * lipshitz * np.power(gamma, iterations)\n",
        "  denominator = (strong * num_examples * (1 - np.power(gamma, iterations))) * ((np.sqrt(np.log(1 / delta) + epsilon)) - np.sqrt(np.log(1 / delta)))\n",
        "  return numerator / denominator\n",
        "\n",
        "def publish(rng, W, sigma):\n",
        "  \"\"\"Publishing function which adds Gaussian noise with scale sigma.\"\"\"\n",
        "  return W + sigma * random.normal(rng, W.shape)\n",
        "\n",
        "def accuracy(W, X, y):\n",
        "  \"\"\"Computes the model accuracy given a dataset.\"\"\"\n",
        "  y_hat = (predict(W, X) > 0.5).astype(np.int32)\n",
        "  return np.mean(y_hat == y)\n",
        "\n",
        "def delete_index(idx, *args):\n",
        "  \"\"\"Deletes index `idx` from each of args (assumes they all have same shape).\"\"\"\n",
        "  mask = np.eye(len(args[0]))[idx] == 0.\n",
        "  return (arg[mask] for arg in args)\n",
        "\n",
        "def append_datum(data, *args):\n",
        "  return (np.concatenate((arg, datum)) for arg, datum in zip(args, data))\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  rng = random.PRNGKey(0)\n",
        "\n",
        "  num_train = 1000\n",
        "  num_test = 200\n",
        "  num_updates = 25\n",
        "\n",
        "  init_iterations = 1000\n",
        "  update_iterations = 25\n",
        "\n",
        "  l2 = 0.05\n",
        "  strong = l2\n",
        "  smooth = 4 - l2\n",
        "  diameter = 2\n",
        "  lipshitz = 1 + l2\n",
        "\n",
        "  epsilon = 5\n",
        "  delta = 1 / (num_train ** 2)\n",
        "\n",
        "  # Two dimensional Gaussian points with label 1 if above Y = 0 and 0 otherwise\n",
        "  X = random.normal(rng, shape=(num_train, 2)) # (num_train, 2)\n",
        "  y = (X[:, 0] > 0.).astype(np.int32) # (num_train,)\n",
        "\n",
        "  X_test = random.normal(rng, shape=(num_test, 2)) # (num_test, 2)\n",
        "  y_test = (X_test[:, 0] > 0.).astype(np.int32) # (num_test,)\n",
        "\n",
        "  W = np.ones((X.shape[1],)) # (2,)\n",
        "  W = unit_projection(W)\n",
        "  W = train(W, X, y, l2, init_iterations)\n",
        "\n",
        "  # Delete first row `num_updates` times in sequence\n",
        "  updates = [lambda X, y: delete_index(0, X, y) for i in range(num_updates)]\n",
        "  train_fn = lambda W, X, y: train(W, X, y, l2, update_iterations)\n",
        "\n",
        "  print('Processing updates...')\n",
        "  W, X, y = process_updates(W, X, y, updates, train_fn)\n",
        "  print('Accuracy: {:.4f}\\n'.format(accuracy(W, X_test, y_test)))\n",
        "\n",
        "  sigma = compute_sigma(num_train, update_iterations, lipshitz, strong, epsilon, delta)\n",
        "  print('Epsilon: {}, Delta: {}, Sigma: {:.4f}'.format(epsilon, delta, sigma))\n",
        "\n",
        "  temp, rng = random.split(rng)\n",
        "  W = publish(temp, W, sigma)\n",
        "  print('Accuracy (published): {:.4f}'.format(accuracy(W, X_test, y_test)))"
      ]
    }
  ]
}
